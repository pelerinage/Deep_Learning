{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4786871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf7be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14) \n",
    "plt.rc('legend', fontsize=14) \n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "import sklearn\n",
    "from tensorflow import keras\n",
    "# Common imports \n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "# to make the output stable across runs \n",
    "np.random.seed(42) \n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "mpl.rc('axes', labelsize=14) \n",
    "mpl.rc('xtick', labelsize=12) \n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf76ad",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "Select and download a data set from the site http://www.manythings.org/anki Choose a language: French, German, Russian, Hebrew, Portugue, or any other language with a large data set. Do not use Spanish, since our class examples already use Spanish. \n",
    "\n",
    "a)\tIdentify the directory where the dataset is downloaded. Examine that file. Report a few initial lines. Write a preprocessing code that will clean downloaded data so that you could transform it into “clean” pairs of sentences in English and the selected language. Create datasets with some 50% of language pairs devoted to training, 45% to validation and 15% to testing.\n",
    "\n",
    "b)\tWork with a vocabulary of 2000 words and sentences of maximal length of 70. Train an encoder-decoder model with attention on your dataset. First test the model with 2 epochs and then run it with 20. On Google Colab with GPU run time, that will take approximately 30 minutes. Illustrate translation ability of your model with 10 sentences of random length from your test dataset.\n",
    "\n",
    "c)\tFind an API on the Web that would allow you to calculate the BLUE scores.  Use sentences in your test dataset. Use you trained model to produce “your” own translations of test sentences. Use the BLUE score API to assess the BLUE score of your model.  Instead of BLUE score you are welcome to use some other techniques for automated measurement of the quality of translations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38bf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate downloaded french english text file\n",
    "path_to_file = '/Users/mpellegrin008/Documents/VS Code/CSCI89_Deep Learning/fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3692d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf3a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, French]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "    np.random.seed(42)  # extra code\n",
    "    np.random.shuffle(word_pairs)\n",
    "\n",
    "    \n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3054a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this seems kind of expensive . <end>\n",
      "<start> ca a l air plutot cher . <end>\n",
      "<start> cc by . france attribution tatoeba . org ck sacredceltic <end>\n"
     ]
    }
   ],
   "source": [
    "sentences_en, sentences_fr, src = create_dataset(path_to_file, 1000000)\n",
    "print(sentences_en[-1])\n",
    "print(sentences_fr[-1])\n",
    "print(src[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b1498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227815\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89fa53",
   "metadata": {},
   "source": [
    "b) Work with a vocabulary of 2000 words and sentences of maximal length of 70. Train an encoder-decoder model with attention on your dataset. First test the model with 2 epochs and then run it with 20. On Google Colab with GPU run time, that will take approximately 30 minutes. Illustrate translation ability of your model with 10 sentences of random length from your test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1271a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 10:55:55.586346: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2023-11-18 10:55:55.586384: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-11-18 10:55:55.586391: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-11-18 10:55:55.586437: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-18 10:55:55.586462: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-11-18 10:55:56.154085: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "#vectorize data\n",
    "\n",
    "vocab_size = 2000\n",
    "max_length = 70\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_fr = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_fr.adapt([f\"startofseq {s} endofseq\" for s in sentences_fr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b67fdd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'start', 'end', 'i', 'you', 'to', 'the', 't', 'a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23197c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'end', 'startofseq', 'start', 'endofseq', 'je', 'a', 'de', 'pas']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_fr.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb01574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113907.5\n",
      "193642.75\n",
      "34172.25\n"
     ]
    }
   ],
   "source": [
    "# Create datasets with some 50% of language pairs devoted to training, 35% to validation and 15% to testing\n",
    "num_train = len(sentences_en)*.5\n",
    "num_val = len(sentences_en)*.35\n",
    "num_test = len(sentences_en) - num_train - num_val\n",
    "print(num_train)\n",
    "print(num_val+num_train)\n",
    "print(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d8c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:113907])\n",
    "X_valid = tf.constant(sentences_en[113907:193642])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_fr[:113907]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_fr[113907:193642]])\n",
    "Y_train = text_vec_layer_fr([f\"{s} endofseq\" for s in sentences_fr[:113907]])\n",
    "Y_valid = text_vec_layer_fr([f\"{s} endofseq\" for s in sentences_fr[113907:193642]])\n",
    "X_test = tf.constant(sentences_en[193642:])\n",
    "Y_test = text_vec_layer_fr([f\"{s} endofseq\" for s in sentences_fr[193642:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cad1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the encoder decoder model with attention\n",
    "\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db865cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_fr(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9efddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32289334",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de685f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = tf.keras.layers.Attention()\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(attention_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d434151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 11:11:08.121216: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3560/3560 [==============================] - ETA: 0s - loss: 3.7176 - accuracy: 0.3777"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 11:21:10.163868: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3560/3560 [==============================] - 773s 216ms/step - loss: 3.7176 - accuracy: 0.3777 - val_loss: 3.3995 - val_accuracy: 0.4197\n",
      "Epoch 2/10\n",
      "3560/3560 [==============================] - 768s 216ms/step - loss: 3.5006 - accuracy: 0.4053 - val_loss: 3.4180 - val_accuracy: 0.4195\n",
      "Epoch 3/10\n",
      "3560/3560 [==============================] - 786s 221ms/step - loss: 3.2019 - accuracy: 0.4423 - val_loss: 3.1043 - val_accuracy: 0.4543\n",
      "Epoch 4/10\n",
      "3560/3560 [==============================] - 775s 218ms/step - loss: 2.9411 - accuracy: 0.4762 - val_loss: 2.8868 - val_accuracy: 0.4815\n",
      "Epoch 5/10\n",
      "3560/3560 [==============================] - 778s 219ms/step - loss: 2.7523 - accuracy: 0.4981 - val_loss: 2.7730 - val_accuracy: 0.4953\n",
      "Epoch 6/10\n",
      "3560/3560 [==============================] - 782s 220ms/step - loss: 2.5659 - accuracy: 0.5207 - val_loss: 2.4823 - val_accuracy: 0.5302\n",
      "Epoch 7/10\n",
      "3560/3560 [==============================] - 780s 219ms/step - loss: 2.4223 - accuracy: 0.5375 - val_loss: 2.3595 - val_accuracy: 0.5471\n",
      "Epoch 8/10\n",
      "3560/3560 [==============================] - 781s 219ms/step - loss: 2.2769 - accuracy: 0.5576 - val_loss: 2.2641 - val_accuracy: 0.5609\n",
      "Epoch 9/10\n",
      "3560/3560 [==============================] - 779s 219ms/step - loss: 2.1197 - accuracy: 0.5802 - val_loss: 2.0731 - val_accuracy: 0.5895\n",
      "Epoch 10/10\n",
      "3560/3560 [==============================] - 780s 219ms/step - loss: 1.9127 - accuracy: 0.6110 - val_loss: 1.9256 - val_accuracy: 0.6122\n"
     ]
    }
   ],
   "source": [
    "#compile the model and train\n",
    "\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))\n",
    "\n",
    "model.save('encoder_decoder.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bfaf266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to translate test sentences\n",
    "\n",
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en])  # encoder input \n",
    "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_fr.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021460b",
   "metadata": {},
   "source": [
    "c) Find an API on the Web that would allow you to calculate the BLUE scores. Use sentences in your test dataset. Use you trained model to produce “your” own translations of test sentences. Use the BLUE score API to assess the BLUE score of your model. Instead of BLUE score you are welcome to use some other techniques for automated measurement of the quality of translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57b75265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<start> you could count to ten when you were two . <end>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0e8aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 13:27:27.764021: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n",
      "2023-11-18 13:27:27.765290: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n",
      "2023-11-18 13:27:27.766540: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n",
      "2023-11-18 13:27:27.767236: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n",
      "2023-11-18 13:27:27.767868: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n",
      "2023-11-18 13:27:27.768487: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n",
      "2023-11-18 13:27:27.769166: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n",
      "2023-11-18 13:27:27.769804: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n",
      "2023-11-18 13:27:27.770428: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n",
      "2023-11-18 13:27:27.770987: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Attempting to slice scalar input.\n"
     ]
    }
   ],
   "source": [
    "seq2seq_translation = []\n",
    "for i in X_test[0:10]:\n",
    "    if len(seq2seq_translation) < 10:\n",
    "        try:\n",
    "            seq2seq_translation.append(translate(i[7:-5])[:-6])\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a00d6397",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "#create list of translated sentences\n",
    "translations = []\n",
    "for i in range(10):\n",
    "    translations.append(translate(X_test[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f239347b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start tu devrais [UNK] de heures lorsque tu [UNK] end',\n",
       " 'start il est a l [UNK] a l [UNK] end',\n",
       " 'start elle etait [UNK] toute tout end',\n",
       " 'start il a [UNK] de pleuvoir de l [UNK] et les [UNK] end',\n",
       " 'start la plupart de la temps [UNK] de la sante end',\n",
       " 'start nous allons de une [UNK] de [UNK] de [UNK] de ici end',\n",
       " 'start mon [UNK] est [UNK] end',\n",
       " 'start tu es [UNK] end',\n",
       " 'start tom a beaucoup de [UNK] end',\n",
       " 'start elle a [UNK] de beaucoup de [UNK] de [UNK] end']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48514349",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_labels = tf.constant([f\"{s} endofseq\" for s in sentences_fr[193642:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e80f62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define references\n",
    "references = []\n",
    "for i in range(10):\n",
    "    references.append(Y_labels[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "50cbcd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<start> quand tu avais deux ans , tu pouvais compter jusqu a dix . <end> endofseq',\n",
       " b'<start> il vint en bus . <end> endofseq',\n",
       " b'<start> elle avait mal partout . <end> endofseq',\n",
       " b'<start> il s est mis a tomber des cordes . <end> endofseq',\n",
       " b'<start> le vin rouge convient bien a la viande . <end> endofseq',\n",
       " b'<start> nous aurons quelques visiteurs un de ces jours . <end> endofseq',\n",
       " b'<start> le football est mon sport prefere . <end> endofseq',\n",
       " b'<start> tu es une de ces menteuses ! <end> endofseq',\n",
       " b'<start> tom a beaucoup de problemes . <end> endofseq',\n",
       " b'<start> elle depense autant qu elle gagne . <end> endofseq']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bece4259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2821097245439388e-231\n",
      "1.322612729825152e-231\n",
      "1.37484330787042e-231\n",
      "1.2128566900665309e-231\n",
      "1.2183324802375697e-231\n",
      "1.2481913216671088e-231\n",
      "1.396075237825507e-231\n",
      "1.4740564900137075e-231\n",
      "1.4703199416115106e-231\n",
      "1.2882297539194154e-231\n"
     ]
    }
   ],
   "source": [
    "#calculate bleu scores for test sentences\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "for original, translation in zip(references, translations):\n",
    "    print(sentence_bleu(str(original), str(translation))) # uses Bleu-4 for 4-grams by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c85ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2bc6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc9e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a22a6694",
   "metadata": {},
   "source": [
    "# Problem 2. \n",
    "\n",
    "Repeat the analysis of Problem 1 with a transformer model similar to the one discussed in class. To increase the precision, again, like in problem 1, work with a slightly bigger vocabulary of 2000 words and somewhat longer sentences of up to 70 words.\n",
    "Illustrate translation ability of your transformer model with the same sentences you used in Problem 1, section b. Compare with the results obtained with the encoder-decoder with attention model.\n",
    "Compare the BLUE score (or some other measure for the quality of translation) between models in problem 1 and problem 2. Provide any comments, if you have them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "252fbfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113907 training pairs\n",
      "79735 validation pairs\n",
      "34173 test pairs\n"
     ]
    }
   ],
   "source": [
    "# Create datasets with some 50% of language pairs devoted to training, 35% to validation and 15% to testing\n",
    "\n",
    "\n",
    "train_pairs = tuple(zip(sentences_en[:113907], sentences_fr[:113907]))\n",
    "val_pairs = tuple(zip(sentences_en[113907 : 193642], sentences_fr[113907 : 193642]))\n",
    "test_pairs = tuple(zip(sentences_en[193642:], sentences_fr[193642:]))\n",
    "\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17bdfabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vecotrize data and adapt\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\" #should not be needed in french, but just in case\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 2000\n",
    "sequence_length = 70\n",
    "batch_size = 64\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "fra_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_fra_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "fra_vectorization.adapt(train_fra_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49bc69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format and create train and val datasets\n",
    "\n",
    "def format_dataset(eng, fra):\n",
    "    eng = eng_vectorization(eng)\n",
    "    fra = fra_vectorization(fra)\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": eng,\n",
    "            \"decoder_inputs\": fra[:, :-1],\n",
    "        },\n",
    "        fra[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, fra_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    fra_texts = list(fra_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fra_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82b2ac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 70)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 70)\n",
      "targets.shape: (64, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 13:42:50.042144: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "#print shape of train dataset\n",
    "\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d969a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transformer model\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        attention_output = self.attention(query=inputs, value=inputs, key=inputs)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.add = layers.Add()  # instead of `+` to preserve mask\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, use_causal_mask=True\n",
    "        )\n",
    "        out_1 = self.layernorm_1(self.add([inputs, attention_output_1]))\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(self.add([out_1, attention_output_2]))\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(self.add([out_2, proj_output]))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"latent_dim\": self.latent_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "806d0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble transformer model\n",
    "\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6865847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " positional_embedding (Posi  (None, None, 256)            529920    ['encoder_inputs[0][0]']      \n",
      " tionalEmbedding)                                                                                 \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
      " formerEncoder)                                                                                   \n",
      "                                                                                                  \n",
      " model_3 (Functional)        (None, None, 2000)           6303440   ['decoder_inputs[0][0]',      \n",
      "                                                                     'transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9988816 (38.10 MB)\n",
      "Trainable params: 9988816 (38.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1780/1780 [==============================] - 2303s 1s/step - loss: 2.4322 - accuracy: 0.5641 - val_loss: 1.7191 - val_accuracy: 0.6633\n",
      "Epoch 2/10\n",
      "1780/1780 [==============================] - 2334s 1s/step - loss: 1.7183 - accuracy: 0.6733 - val_loss: 1.5187 - val_accuracy: 0.6994\n",
      "Epoch 3/10\n",
      "1780/1780 [==============================] - 2333s 1s/step - loss: 1.5216 - accuracy: 0.7042 - val_loss: 1.4228 - val_accuracy: 0.7163\n",
      "Epoch 4/10\n",
      "1780/1780 [==============================] - 3929s 2s/step - loss: 1.3917 - accuracy: 0.7255 - val_loss: 1.3623 - val_accuracy: 0.7260\n",
      "Epoch 5/10\n",
      "1780/1780 [==============================] - 2252s 1s/step - loss: 1.2951 - accuracy: 0.7407 - val_loss: 1.3271 - val_accuracy: 0.7313\n",
      "Epoch 6/10\n",
      "1780/1780 [==============================] - 3302s 2s/step - loss: 1.2163 - accuracy: 0.7534 - val_loss: 1.2986 - val_accuracy: 0.7335\n",
      "Epoch 7/10\n",
      "1780/1780 [==============================] - 2240s 1s/step - loss: 1.1544 - accuracy: 0.7638 - val_loss: 1.2654 - val_accuracy: 0.7423\n",
      "Epoch 8/10\n",
      "1780/1780 [==============================] - 2281s 1s/step - loss: 1.1031 - accuracy: 0.7731 - val_loss: 1.2657 - val_accuracy: 0.7425\n",
      "Epoch 9/10\n",
      "1780/1780 [==============================] - 2305s 1s/step - loss: 1.0575 - accuracy: 0.7811 - val_loss: 1.2599 - val_accuracy: 0.7438\n",
      "Epoch 10/10\n",
      "1780/1780 [==============================] - 2288s 1s/step - loss: 1.0179 - accuracy: 0.7885 - val_loss: 1.2413 - val_accuracy: 0.7494\n"
     ]
    }
   ],
   "source": [
    "#train transformer model\n",
    "\n",
    "epochs = 10 \n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "history1 = transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
    "transformer.save('transformer.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b95c64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decoder for test sentences\n",
    "\n",
    "fra_vocab = fra_vectorization.get_vocabulary()\n",
    "fra_index_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = fra_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = fra_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "29074e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> you could count to ten when you were two . <end>\n",
      "[start] end  un on un [UNK] [UNK] [UNK] a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] end a un [UNK] end\n",
      "b'<start> quand tu avais deux ans , tu pouvais compter jusqu a dix . <end> endofseq'\n",
      "   \n",
      "<start> he came by bus . <end>\n",
      "[start] end  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] le [UNK] [UNK]\n",
      "b'<start> il vint en bus . <end> endofseq'\n",
      "   \n",
      "<start> she was aching all over . <end>\n",
      "[start] end  [UNK] [UNK] [UNK] le [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] le [UNK] [UNK] end end [UNK] end\n",
      "b'<start> elle avait mal partout . <end> endofseq'\n",
      "   \n",
      "<start> it began to rain cats and dogs . <end>\n",
      "[start] end  un [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] le [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] le\n",
      "b'<start> il s est mis a tomber des cordes . <end> endofseq'\n",
      "   \n",
      "<start> red wine goes well with meat . <end>\n",
      "[start] end  [UNK] [UNK] un [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] end [UNK] le\n",
      "b'<start> le vin rouge convient bien a la viande . <end> endofseq'\n",
      "   \n",
      "<start> we will have some visitors one of these days . <end>\n",
      "[start] de ces jours nous en [UNK] un de ces jours end  [UNK] [UNK] [UNK] [UNK] [UNK] end il [UNK]\n",
      "b'<start> nous aurons quelques visiteurs un de ces jours . <end> endofseq'\n",
      "   \n",
      "<start> my favorite sport is soccer . <end>\n",
      "[start] end  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] le [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] le\n",
      "b'<start> le football est mon sport prefere . <end> endofseq'\n",
      "   \n",
      "<start> you are such a liar ! <end>\n",
      "[start] end  un [UNK] un [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "b'<start> tu es une de ces menteuses ! <end> endofseq'\n",
      "   \n",
      "<start> tom has a lot of problems . <end>\n",
      "[start] end  un un [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] end [UNK] end le end \n",
      "b'<start> tom a beaucoup de problemes . <end> endofseq'\n",
      "   \n",
      "<start> she spends as much money as she earns . <end>\n",
      "[start] end  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] end le end end    \n",
      "b'<start> elle depense autant qu elle gagne . <end> endofseq'\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "\n",
    "#append translated sentences to a list\n",
    "translations1 = []\n",
    "for i in range(10):\n",
    "    input_sentence = test_eng_texts[i]\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    translations1.append(translated)\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print(references[i])\n",
    "    print(\"   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c58ae3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0350003944289303e-231\n",
      "9.594503055152632e-232\n",
      "9.788429383461836e-232\n",
      "9.72161026064145e-232\n",
      "9.958726787503715e-232\n",
      "1.1200407237786664e-231\n",
      "9.65701126654974e-232\n",
      "9.65701126654974e-232\n",
      "1.0244914152188952e-231\n",
      "1.0216652050829071e-231\n"
     ]
    }
   ],
   "source": [
    "for original, translation in zip(references, translations1):\n",
    "    print(sentence_bleu(str(original), str(translation))) # uses Bleu-4 for 4-grams by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df2179",
   "metadata": {},
   "source": [
    "Contrary to our initial expectations, the transformer model does not perform better than the encoder-decoder with attention.  This is likely due to the limited number of epochs that we were able to train the model for, as well as the limited vocabulary size.  Additionally, we may need to improve the pre-processing of the sentences. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
